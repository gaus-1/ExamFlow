from __future__ import annotations

from core.container import Container

try:
    from telegram_bot.bot_handlers import (
        db_create_enhanced_prompt as _legacy_create_enhanced_prompt,  # type: ignore
        db_add_user_message_to_session as _legacy_add_user_message,  # type: ignore
        db_add_assistant_message_to_session as _legacy_add_assistant_message,  # type: ignore
        db_get_or_create_chat_session as _legacy_get_or_create_chat_session,  # type: ignore
    )
except Exception:
    def _legacy_create_enhanced_prompt(user_message: str, session) -> str:  # type: ignore
        return user_message

    def _legacy_add_user_message(session, message):  # type: ignore
        return None

    def _legacy_add_assistant_message(session, message):  # type: ignore
        return None

    def _legacy_get_or_create_chat_session(telegram_user, django_user=None):  # type: ignore
        return None


# –§–∞—Å–∞–¥–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (—Ç–æ–Ω–∫–∏–µ –æ–±—ë—Ä—Ç–∫–∏)

def get_or_create_chat_session(telegram_user, django_user=None):  # type: ignore
    return _legacy_get_or_create_chat_session(telegram_user, django_user)


def create_enhanced_prompt(user_message: str, session) -> str:
    return _legacy_create_enhanced_prompt(user_message, session)


def add_user_message_to_session(session, message) -> None:
    _legacy_add_user_message(session, message)


def add_assistant_message_to_session(session, message) -> None:
    _legacy_add_assistant_message(session, message)


def get_ai_response(prompt: str, task_type: str = 'chat', user=None, task=None) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç AI —Å –ø–∞–º—è—Ç—å—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        from telegram_bot.memory import BotContextManager
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π AI API –¥–ª—è –±–æ—Ç–∞ (–ø—Ä—è–º–æ–π Gemini)
        import google.generativeai as genai
        from django.conf import settings
        
        api_key = getattr(settings, 'GEMINI_API_KEY', '')
        if not api_key:
            return '–°–µ—Ä–≤–∏—Å –ò–ò –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Gemini
        genai.configure(api_key=api_key)  # type: ignore
        model = genai.GenerativeModel('gemini-1.5-flash')  # type: ignore
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_manager = BotContextManager()
        
        # –ü–æ–ª—É—á–∞–µ–º ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_id = getattr(user, 'telegram_id', None) if user else None
        if not user_id and hasattr(user, 'id'):
            user_id = user.id
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        if user_id:
            prompt_with_context = context_manager.format_context_for_ai(user_id, prompt)
        else:
            prompt_with_context = prompt
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å —Ä–æ–ª—å—é ExamFlow
        system_prompt = """–¢—ã - ExamFlow AI, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –ï–ì–≠ –∏ –û–ì–≠.

–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—à—å—Å—è –Ω–∞:
üìê –ú–∞—Ç–µ–º–∞—Ç–∏–∫–µ (–ø—Ä–æ—Ñ–∏–ª—å–Ω–∞—è –∏ –±–∞–∑–æ–≤–∞—è, –û–ì–≠) - —É—Ä–∞–≤–Ω–µ–Ω–∏—è, —Ñ—É–Ω–∫—Ü–∏–∏, –≥–µ–æ–º–µ—Ç—Ä–∏—è, –∞–ª–≥–µ–±—Ä–∞
üìù –†—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (–ï–ì–≠ –∏ –û–ì–≠) - –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞, –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è, —Å–æ—á–∏–Ω–µ–Ω–∏—è, –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞

–°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è:
- –î—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π
- –ò–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–π —É–º–µ—Å—Ç–Ω—ã–µ —à—É—Ç–∫–∏ –ø–æ —Ç–µ–º–µ
- –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–π—Å—è –∫–∞–∫ "ExamFlow AI" 
- –ü–æ–º–æ–≥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –ø–æ—à–∞–≥–æ–≤—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏
- –í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–∞ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
- –ï—Å–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ç–µ–º—É - —Ä–∞–∑–≤–∏–≤–∞–π –µ—ë –¥–∞–ª—å—à–µ
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–≤–æ–∏–º –ø—Ä–µ–¥–º–µ—Ç–∞–º - –≤–µ–∂–ª–∏–≤–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤—å –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É –∏–ª–∏ —Ä—É—Å—Å–∫–∏–π

–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, –º–∞–∫—Å–∏–º—É–º 400 —Å–ª–æ–≤."""

        full_prompt = f"{system_prompt}\n\n{prompt_with_context}"
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        response = model.generate_content(full_prompt)
        
        if response.text:
            answer = response.text.strip()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if user_id:
                context_manager.add_message(user_id, prompt, is_user=True)
                context_manager.add_message(user_id, answer, is_user=False)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è Telegram
            if len(answer) > 4000:
                answer = answer[:4000] + "..."
            return answer
        else:
            return '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.'
    except Exception as e:
        # type: ignore
        import logging  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä, –µ—Å–ª–∏ –Ω–µ –±—ã–ª –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —Ä–∞–Ω–µ–µ
        logging.error(f"–û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {e}")
        return f'–û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞: –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ'
